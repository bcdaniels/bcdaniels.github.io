<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bryan Daniels</title><link href="https://www.public.asu.edu/~bdaniel6/" rel="alternate"></link><link href="https://www.public.asu.edu/~bdaniel6/feeds/all.atom.xml" rel="self"></link><id>https://www.public.asu.edu/~bdaniel6/</id><updated>2022-04-11T00:00:00-07:00</updated><entry><title>Ma Mignonne</title><link href="https://www.public.asu.edu/~bdaniel6/ma-mignonne.html" rel="alternate"></link><published>2015-08-08T00:00:00-07:00</published><updated>2015-08-08T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2015-08-08:/~bdaniel6/ma-mignonne.html</id><summary type="html">&lt;p&gt;Thinking swarm:&lt;br&gt;
Choices form&lt;br&gt;
from the all.&lt;br&gt;
Monkeys brawl,&lt;br&gt;
cliques ensue;&lt;br&gt;
What cells do&lt;br&gt;
proteins guide;&lt;br&gt;
To decide&lt;br&gt;
neurons spike.&lt;br&gt;
And unlike&lt;br&gt;
in stat phys …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Thinking swarm:&lt;br&gt;
Choices form&lt;br&gt;
from the all.&lt;br&gt;
Monkeys brawl,&lt;br&gt;
cliques ensue;&lt;br&gt;
What cells do&lt;br&gt;
proteins guide;&lt;br&gt;
To decide&lt;br&gt;
neurons spike.&lt;br&gt;
And unlike&lt;br&gt;
in stat phys&lt;br&gt;
where there is&lt;br&gt;
much that’s known&lt;br&gt;
having grown&lt;br&gt;
infinite&lt;br&gt;
or when it&lt;br&gt;
is the same&lt;br&gt;
little game&lt;br&gt;
played by all,&lt;br&gt;
what of small&lt;br&gt;
and diverse&lt;br&gt;
groups?  How terse&lt;br&gt;
and concise&lt;br&gt;
will suffice&lt;br&gt;
to detail&lt;br&gt;
how you scale&lt;br&gt;
and perform,&lt;br&gt;
thinking swarm?&lt;/p&gt;
&lt;p&gt;Inspired by &lt;a href="http://www.clementmarot.com/MaMignonne.htm"&gt;Le Ton Beau de Marot&lt;/a&gt;&lt;/p&gt;</content><category term="Other"></category></entry><entry><title>Collective Cognition</title><link href="https://www.public.asu.edu/~bdaniel6/collective-cognition.html" rel="alternate"></link><published>2014-09-01T00:00:00-07:00</published><updated>2014-09-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2014-09-01:/~bdaniel6/collective-cognition.html</id><summary type="html">&lt;p&gt;In the summer of 2014, I helped organize a working group at the Santa Fe Institute to explore the nascent field of collective cognition. We …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the summer of 2014, I helped organize a working group at the Santa Fe Institute to explore the nascent field of collective cognition. We had fun chatting with thinkers from diverse fields, and this video summarizes some resulting thoughts and speculations about what collective cognition is all about and where it's going.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://vimeo.com/111008805"&gt;&lt;img alt="alt text" src="https://www.public.asu.edu/~bdaniel6/images/collectiveCognitionVideoStill.jpg"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="What I've Been Thinking About"></category><category term="Video"></category></entry><entry><title>Complex Systems for High School Students</title><link href="https://www.public.asu.edu/~bdaniel6/complex-systems-for-high-school-students.html" rel="alternate"></link><published>2014-08-01T00:00:00-07:00</published><updated>2014-08-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2014-08-01:/~bdaniel6/complex-systems-for-high-school-students.html</id><summary type="html">&lt;p&gt;A ragtag team has emerged here in &lt;a href="http://c4.discovery.wisc.edu"&gt;C4&lt;/a&gt; to develop &lt;a href="http://c4.discovery.wisc.edu/education/highschool/"&gt;interactive activities&lt;/a&gt; that spread the ideas of transdisciplinary science to a broader audience.  As of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A ragtag team has emerged here in &lt;a href="http://c4.discovery.wisc.edu"&gt;C4&lt;/a&gt; to develop &lt;a href="http://c4.discovery.wisc.edu/education/highschool/"&gt;interactive activities&lt;/a&gt; that spread the ideas of transdisciplinary science to a broader audience.  As of Fall 2014, we are looking to grow our team: Any interested University of Wisconsin students  should &lt;a href="mailto:bdaniels@discovery.wisc.edu"&gt;contact me&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://vimeo.com/105179628"&gt;&lt;img alt="C4 outreach video still" src="https://www.public.asu.edu/~bdaniel6/images/c4outreachVideoStill.jpg"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Sparse Coding in a Social System</title><link href="https://www.public.asu.edu/~bdaniel6/sparse-coding-in-a-social-system.html" rel="alternate"></link><published>2012-07-01T00:00:00-07:00</published><updated>2012-07-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2012-07-01:/~bdaniel6/sparse-coding-in-a-social-system.html</id><summary type="html">&lt;p&gt;How do we understand our social environment?  If you're like me, when you think about people you know, you probably think in terms of groups …&lt;/p&gt;</summary><content type="html">&lt;p&gt;How do we understand our social environment?  If you're like me, when you think about people you know, you probably think in terms of groups: immediate family, friends from high school, colleagues.  Each group contains people who tend to show up together.  If I try to remember, for instance, who attended my wedding, I first think of these groups.  This type of organization makes sense in terms of something psychologists call "chunking."  This is the idea that people tend to recall complex information by breaking it into logical parts.  This is why you can more easily remember the letters of a long password if it consists of familiar words.&lt;/p&gt;
&lt;p&gt;When I started collaborating with &lt;a href="http://c4.discovery.wisc.edu/scientists/flack"&gt;Jessica Flack&lt;/a&gt; and &lt;a href="http://c4.discovery.wisc.edu/scientists/krakauer"&gt;David Krakauer&lt;/a&gt;, then at the Santa Fe Institute, we wondered if similar ideas would be useful in the context of a social environment for which we had lots of detailed data: the outbreaks of conflict in a society of macaque monkeys.  The fighting behavior of these 47 primates indicates that they have knowledge of an established social hierarchy and that they are able to make strategic decisions based on this knowledge.  Given this, we wondered: If a monkey is trying to anticipate common patterns of conflict, would it make sense to memorize the groups who commonly appeared together?  And would such a strategy work as well as, say, remembering how often every pair of individuals were simultaneously aggressive?&lt;/p&gt;
&lt;p&gt;In answering these questions, we were inspired by the study of sparse coding in neuroscience.  Like thinking in terms of groups at a wedding, it turns out that common visual scenes can be described more efficiently in terms of larger shapes and lines than by describing them pixel by pixel.  Neuroscientists have discovered that neurons in the brain represent what you see in terms of this kind of sparse code.  As you read this sentence, your visual cortex is responding to the lines that make up the text as one of the first steps in the process that allows you to recognize each word.  Applying the same logic to the conflict data set, we used a machine learning algorithm to discover the groups of macaques that could most succinctly represent the fights that occurred.  &lt;/p&gt;
&lt;p&gt;We found that this was a good way of representing the patterns of conflict, and that remembering just the groups found by sparse coding could be used to make predictions about who was likely to show up in a given fight; the performace was as good as the alternate approach of remembering the relationships of every pair of invidiauals.  The sparse groups were related to known categories, such as kin groups, and their size told us that most relevant social structure happened on a scale of around 2 or 3 individuals.&lt;/p&gt;
&lt;p&gt;Finally, in terms of information theory, these representations are a form of compression.  Instead of having to remember every possible pattern of conflict, an individual can remember just the sparse groups and still make accurate predictions.  In fact, we can estimate how much information an individual would have to remember.  By measuring the effects of varying the degree of compression, we estimate that a monkey would have to remember about 1000 bits of social information to make optimal predictions about future fights.&lt;/p&gt;
&lt;p&gt;For more information, check out the &lt;a href="http://www.news.wisc.edu/20960"&gt;University of Wisconsin–Madison press release&lt;/a&gt; and &lt;a href="../DanKraFla12.pdf"&gt;our full published paper&lt;/a&gt; [pdf].&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Statistical Inference in Complex Systems</title><link href="https://www.public.asu.edu/~bdaniel6/statistical-inference-in-complex-systems.html" rel="alternate"></link><published>2012-06-01T00:00:00-07:00</published><updated>2022-04-11T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2012-06-01:/~bdaniel6/statistical-inference-in-complex-systems.html</id><summary type="html">&lt;p&gt;For a broad introduction in my own words to the research we do here, check out &lt;strong&gt;&lt;a href="https://www.public.asu.edu/~bdaniel6/audio/[tindeck.com]-Chris_Bocast-Episode_3-The_Center_for_Complexity_and_Collective_Computation.mp3"&gt;this podcast highlighting the Center for Complexity and Collective …&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;For a broad introduction in my own words to the research we do here, check out &lt;strong&gt;&lt;a href="https://www.public.asu.edu/~bdaniel6/audio/[tindeck.com]-Chris_Bocast-Episode_3-The_Center_for_Complexity_and_Collective_Computation.mp3"&gt;this podcast highlighting the Center for Complexity and Collective Computation&lt;/a&gt;&lt;/strong&gt;, Episode 3 of the series &lt;a href="http://itunes.apple.com/us/itunes-u/episode-3-center-for-complexity/id531782587?i=117819414"&gt;WID: The Challenge of Transdisciplinary Science&lt;/a&gt; produced by &lt;a href="http://www.chrisbocast.com"&gt;Chris Bocast&lt;/a&gt;.&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Predicting Biological Network Output</title><link href="https://www.public.asu.edu/~bdaniel6/predicting-biological-network-output.html" rel="alternate"></link><published>2009-09-01T00:00:00-07:00</published><updated>2009-09-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2009-09-01:/~bdaniel6/predicting-biological-network-output.html</id><content type="html">&lt;p&gt;More to come.  For now: &lt;a href="https://www.public.asu.edu/~bdaniel6/pdfs/qbio2009_presentation.pdf"&gt;q-bio conference presentation&lt;/a&gt; [pdf]&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Mandelbrot</title><link href="https://www.public.asu.edu/~bdaniel6/mandelbrot.html" rel="alternate"></link><published>2009-08-21T00:00:00-07:00</published><updated>2009-08-21T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2009-08-21:/~bdaniel6/mandelbrot.html</id><content type="html">&lt;p&gt;&lt;a href="https://www.public.asu.edu/~bdaniel6/images/Mandelbrot2.gif"&gt;Mandelbrot&lt;/a&gt;&lt;/p&gt;</content><category term="Other"></category></entry><entry><title>Biological Neural Networks</title><link href="https://www.public.asu.edu/~bdaniel6/biological-neural-networks.html" rel="alternate"></link><published>2009-02-01T00:00:00-07:00</published><updated>2009-02-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2009-02-01:/~bdaniel6/biological-neural-networks.html</id><summary type="html">&lt;p&gt;In biology, we often find large collections of relatively simple interacting elements that combine to create complicated structures with complex behavior. Neural networks and gene …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In biology, we often find large collections of relatively simple interacting elements that combine to create complicated structures with complex behavior. Neural networks and gene interaction networks are two such examples.&lt;/p&gt;
&lt;p&gt;The communities studying these systems share a common problem: the available measurements are not able to resolve all the individual components or interactions (nor do we expect them to be able to do so soon, with the large numbers of components typically involved). We are left in a situation in which we are unable to fill in the middle-scale details. We know how a single neuron fires, and how a single gene transcription factor can alter the rate of the production of proteins. We also have a feeling for the broad behavior of information transfer in the brain, and the switching abilities of entire gene networks. But lacking is an understanding of exactly how the components fit into the whole.&lt;/p&gt;
&lt;p&gt;The problem, then, is taking incomplete information about the behavior of a network and inferring something about how it works. The goal is to be able to at least match the output of a network with a model, and at most to have a full understanding of the individual players and interactions among them that produce the behavior in the actual system.&lt;/p&gt;
&lt;p&gt;For my graduate A exam, Veit Elser suggested that I look into methods that the neuroscience community uses to deal with the fact that studies of biological neural networks are limited by the amount of simultaneous data they can record.  The question got me thinking about parameter search algorithms, prior beliefs in Bayesian statistics, and effective network models.  You can read my response &lt;a href='AExamNeurosciencePaper.pdf'&gt;here&lt;/a&gt; [pdf].&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Sloppy Models</title><link href="https://www.public.asu.edu/~bdaniel6/sloppy-models.html" rel="alternate"></link><published>2008-10-01T00:00:00-07:00</published><updated>2008-10-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2008-10-01:/~bdaniel6/sloppy-models.html</id><summary type="html">&lt;p&gt;&lt;img alt="sloppy models" src="https://www.public.asu.edu/~bdaniel6/images/sloppyModels.png"&gt;&lt;/p&gt;
&lt;p&gt;The biochemistry happening inside each one of your cells is amazingly complex.  As an important example, take gene regulation.  In the process of transcription and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="sloppy models" src="https://www.public.asu.edu/~bdaniel6/images/sloppyModels.png"&gt;&lt;/p&gt;
&lt;p&gt;The biochemistry happening inside each one of your cells is amazingly complex.  As an important example, take gene regulation.  In the process of transcription and translation, proteins are constructed from the information in your DNA's genetic code.  This process is regulated so that the cell can make more or less of a certain protein when it needs to (responding to, for example, the presence of a hormone in the bloodstream).  The problem becomes more complicated when you realize that some proteins themselves regulate the creation of other proteins; we could find that protein A upregulates the creation of protein B, which downregulates the creation of protein C, and so on.  In fact, huge networks of interacting genes and proteins are routinely studied in systems biology.&lt;/p&gt;
&lt;p&gt;Understanding these large biochemical networks is a big challenge.  For one, it's hard for experimentalists to measure what's going on inside a tiny living cell.  Still, they can (painstakingly) discover which proteins are connected to which others (If I don't let the cell make protein A, do I still see protein B?), and a network 'topology' is gradually built up.&lt;/p&gt;
&lt;p&gt;But what if we actually want to predict &lt;i&gt;how much&lt;/i&gt; of a certain protein will be made under certain conditions (say, the addition of a drug)?  Then we have to know not only the network topology (protein A upregulates the production of protein B), but specific numbers for each connection (protein A increases the rate of creation of protein B by 2.5x), and specific numbers for the rates involved (one copy of protein A is created every 5 seconds).  If we're trying to model the network, we need to set numbers for lots of these parameters.&lt;/p&gt;
&lt;p&gt;But these parameters are even harder to measure than the topology: Asking the question of how much protein is present is much more difficult than asking whether the protein is present.  So we have to deal with limited information.  We may only know the concentrations of two of the proteins in our network, and have only vague ideas about the concentrations of ten others.  Then our group is tasked with finding values for 50 parameters that produce a reasonable fit to the available data, so that we can make a prediction about what will happen in other, unmeasured conditions.&lt;/p&gt;
&lt;p&gt;As you might imagine, this problem is generally ill-constrained: there are lots of different ways you can set your parameters and still find model output that agrees with the available data.  Some parameters could be intrinsically unimportant to what you measured.  Some sets of parameters could compensate for each other; for example, raising one rate and lowering another might leave the output unchanged.  (We say that there are lots of 'sloppy' directions in parameter space in which you can move without changing the model output.)  And at first glance, it seems audacious to think that anything useful could come out of all of this.  If we don't know our parameters very well, how can we hope to make valid predictions?&lt;/p&gt;
&lt;p&gt;But it turns out that the situation is not so bleak.  If we keep track of all of the parameter sets that work to fit the experimental data, we can plug them in and see what output each of them produces for an unmeasured condition.  And we find that (well, Ryan Gutenkunst found that) oftentimes the outputs of all these possible parameter sets are alike enough that we can still make a prediction with some confidence.  In fact, even if we imagined doing experiments to reasonably measure each of the individual parameters, we couldn't do much better.  This is saying that the experimental data still constrain the predictions we care about, even if they don't constrain the parameter values.&lt;/p&gt;
&lt;p&gt;There are lots of other interesting questions you can imagine asking about these 'sloppy models.'  Can these models be systematically simplified to contain fewer parameters?  Can other types of measurements (say, of fluctuations) better constrain parameter values?  If organisms evolve by changing parameters, can 'sloppiness' help us understand evolution?  You can learn more at my advisor's website: &lt;a href=http://www.lassp.cornell.edu/sethna/Sloppy/index.html&gt;Sloppy Models&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Fermi Liquids</title><link href="https://www.public.asu.edu/~bdaniel6/fermi-liquids.html" rel="alternate"></link><published>2008-09-01T00:00:00-07:00</published><updated>2008-09-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2008-09-01:/~bdaniel6/fermi-liquids.html</id><summary type="html">&lt;p&gt;Why do metals behave like metals?  Why are they good conductors of both electricity and heat?  The answer to these questions has been known for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Why do metals behave like metals?  Why are they good conductors of both electricity and heat?  The answer to these questions has been known for a long time: metals have free electrons that can easily move through the material, carrying both electrical charge and thermal energy.  A more puzzling fact has been that these electrons can be treated as if they move independently of each other -- under this assumption, the theory is considerably simpler, and yet it works impressively well.  But why?  Why can we treat conduction electrons as a noninteracting "electron gas," especially when we know that the electrons (like any other charged particles) should repel each other with a powerful Coulomb force?&lt;/p&gt;
&lt;p&gt;This mystery was also eventually solved (creating a theory now known as Fermi liquids), and can be explained nicely in terms of &lt;i&gt;renormalization group&lt;/i&gt; ideas.  In Jim Sethna's Statistical Physics class, a group of us (Mark Transtrum, Johannes Lischner, Duane Loh, and I) took on the task of explaining these ideas to the rest of the class.  We delved into imaginary time path integrals and Grassmann numbers, and eventually distilled a whirlwind (but hopefully coherent) summary.&lt;/p&gt;
&lt;p&gt;The basic idea is that electrons do feel each other, but they move in collective excitations called 'quasiparticles' that don't interact as strongly.  And even if these quasiparticles feel each other (have nonzero coupling), the renormalization group tells you that the 'effective' coupling gets weaker and weaker as you look at quasiparticles with lower and lower energy.  This means that when you put a small (low energy) electric field across the metal, you get low energy nearly-noninteracting excitations (quasiparticles) that carry current.  This is why the traditional electron gas model works so well.&lt;/p&gt;
&lt;p&gt;Not only that, but this theory predicts how the electron gas theory fails.  As one example, quasiparticles have a different effective mass than electrons, and this holds even as we zoom in on the low energy quasiparticles.  More interestingly, if the quasiparticles attract each other in a specific way (which can happen, for example, if the quasiparticles interact with vibrations in the metal), the effective strength of this attraction &lt;i&gt;grows&lt;/i&gt; as you look at lower and lower energies.  The low energy behavior of the metal is no longer described as an electron gas, and we instead 'flow to a different fixed point' and get behavior like superconductivity, where quasiparticles pair up and flow without resistance through the metal.  Cool!&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Supercoiled DNA</title><link href="https://www.public.asu.edu/~bdaniel6/supercoiled-dna.html" rel="alternate"></link><published>2008-09-01T00:00:00-07:00</published><updated>2008-09-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2008-09-01:/~bdaniel6/supercoiled-dna.html</id><summary type="html">&lt;p&gt;&lt;img alt="a plectoneme" src="https://www.public.asu.edu/~bdaniel6/images/supercoiling.png"&gt;&lt;/p&gt;
&lt;p&gt;The nucleus of each of your cells contains a highly compacted and organized bundle of DNA, which, when stretched out to a line, would span …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="a plectoneme" src="https://www.public.asu.edu/~bdaniel6/images/supercoiling.png"&gt;&lt;/p&gt;
&lt;p&gt;The nucleus of each of your cells contains a highly compacted and organized bundle of DNA, which, when stretched out to a line, would span about a meter.  Yet it all fits in a nucleus about 10 microns across — scaling things up, imagine a garden hose that would encircle the earth; now gather it all up to fit inside a football stadium.&lt;/p&gt;
&lt;p&gt;Besides the problem of getting the DNA to fit inside such a small space, your cell also needs to be able to access the information that resides in the DNA, and to make copies of the entire length of DNA.  If you've had any experience with a garden hose a few meters long, you know it can be a difficult task to straighten it out, without any kinks, the way DNA must be straightened before it can be copied.  Just imagine how hard your task becomes in the football stadium.&lt;/p&gt;
&lt;p&gt;Thankfully, your cell has tools that it uses to deal with the mass of DNA: it winds the DNA into loops using histones, and can make cuts to relieve torsion using topoisomerases.  Still, during the normal operation of the cell, the DNA is likely to get wound up around itself when it gets too twisted.  Like an overtwisted phone cord on an old-fashioned land-line telephone, the DNA becomes "supercoiled."&lt;/p&gt;
&lt;p&gt;Here at Cornell, in Michelle Wang's lab, DNA is being pulled and twisted to determine exactly how it behaves.  They find that it behaves much like you would expect for a rope: if you twist it too much, it wants to jut out to the side and wrap around itself.  Using an optical trap, they can twist a single molecule of DNA while pulling on its ends and watch it suddenly buckle into a supercoiled shape called a plectoneme.   I have been working with Jim Sethna on a computational model of this setup -- we'd like to be able to explain why the transition happens the way it does.&lt;/p&gt;
&lt;p&gt;I'll write more later; for now, here's a &lt;a href="https://www.public.asu.edu/~bdaniel6/images/DNAmovie5.gif"&gt;link&lt;/a&gt; to a movie showing a plectoneme forming.  (A caveat: our dynamics are not physical, but it is still fun to watch).&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Swimming Zebrafish</title><link href="https://www.public.asu.edu/~bdaniel6/swimming-zebrafish.html" rel="alternate"></link><published>2007-07-01T00:00:00-07:00</published><updated>2007-07-01T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2007-07-01:/~bdaniel6/swimming-zebrafish.html</id><summary type="html">&lt;p&gt;As part of the Cornell &lt;a href="www.chaos.cornell.edu/"&gt;IGERT&lt;/a&gt; program in nonlinear science, I  worked on a project with Amina Kinkhabwala, Jordan Atlas, and Dan Brown in an …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As part of the Cornell &lt;a href="www.chaos.cornell.edu/"&gt;IGERT&lt;/a&gt; program in nonlinear science, I  worked on a project with Amina Kinkhabwala, Jordan Atlas, and Dan Brown in an effort to better understand the neural networks underlying swimming in zebrafish.  We attempted to explain an interesting pattern that was recently discovered in experiments in &lt;a href="http://www.nbb.cornell.edu/neurobio/Fetcho/index.htm"&gt;Joe Fetcho's lab&lt;/a&gt;: the neurons that fire during slow swimming are more easily excitable than those that fire only during fast swimming.&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Supersolid Helium</title><link href="https://www.public.asu.edu/~bdaniel6/supersolid-helium.html" rel="alternate"></link><published>2007-05-04T00:00:00-07:00</published><updated>2007-05-04T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2007-05-04:/~bdaniel6/supersolid-helium.html</id><summary type="html">&lt;p&gt;I recently chose to learn about "supersolids" for a term paper in my solid state physics class.  It's an interesting and still very open field …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently chose to learn about "supersolids" for a term paper in my solid state physics class.  It's an interesting and still very open field of research.  Here's my &lt;a href="https://www.public.asu.edu/~bdaniel6/pdfs/Supersolid.pdf"&gt;term paper&lt;/a&gt; [pdf].&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>CTRNN Parameter Space</title><link href="https://www.public.asu.edu/~bdaniel6/ctrnn-parameter-space.html" rel="alternate"></link><published>2006-08-21T00:00:00-07:00</published><updated>2006-08-21T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2006-08-21:/~bdaniel6/ctrnn-parameter-space.html</id><summary type="html">&lt;p&gt;During the summer I worked with &lt;a href="http://mypage.iu.edu/~rdbeer/"&gt;Randy Beer&lt;/a&gt; at &lt;a href="http://www.iub.edu/"&gt;Indiana University&lt;/a&gt; on a project involving Continuous-Time Recurrent Neural Networks (CTRNNs). Simulated on a computer, these …&lt;/p&gt;</summary><content type="html">&lt;p&gt;During the summer I worked with &lt;a href="http://mypage.iu.edu/~rdbeer/"&gt;Randy Beer&lt;/a&gt; at &lt;a href="http://www.iub.edu/"&gt;Indiana University&lt;/a&gt; on a project involving Continuous-Time Recurrent Neural Networks (CTRNNs). Simulated on a computer, these networks consist of any number of nodes, each of which has an internal state that changes in time in a simple but nonlinear way.  When you connect them to each other, the behavior of each node also depends on the states of all the other nodes, and you can thus end up with very complicated behavior.&lt;/p&gt;
&lt;p&gt;Randy's work has explored how, when combined with an evolutionary algorithm, CTRNNs can perform simple cognitive tasks without any other instructions.  For example, a simulated "bug" can independently discover that the best way to walk is by swinging its legs in a certain coordinated way.  The idea is this: 1) make a bunch of random CTRNNs to control your bug, 2) see which ones make your bug crawl the furthest, 3) use these to make new CTRNNs, 4) repeat.  After thousands of generations, this process creates CTRNNs that are amazingly good at making bugs crawl.  Even though the initial, randomly picked CTRNNs produce hopeless random swings of legs and feet, the final CTRNNs beautifully coordinate a smooth walking behavior.  Moreover, networks have been evolved to perform all sorts of tasks, from chemotaxis to simple learning.&lt;/p&gt;
&lt;p&gt;So how does this work?  This is a huge question, with at least two big parts.  First: How do the evolved networks solve the problems they are given?  In the language of dynamical systems, how are the best networks set up to produce the desired trajectories in phase space?  (And even, which trajectories produce the desired behavior?)  These are the problems that Randy and his group have focused on in the past.&lt;/p&gt;
&lt;p&gt;Secondly, we'd also like to know how the evolutionary algorithm accomplishes its goal: How can it find the needle of the solution in the daunting haystack of all possible CTRNNs?  In reality, even with all the successes of evolving CTRNNs that perform perfectly, many evolutionary "searches" turn up emptyhanded, and for reasons that are not well-understood.  There are lots of ad hoc procedures people use when they hit a dead end, but there is no universal way to increase the likelihood of an evolutionary search finding its goal.  My work over the summer set out to change that, or at least to understand the question better.&lt;/p&gt;
&lt;p&gt;One way to look at the problem an evolutionary algorithm faces is to imagine the space of all possible CTRNNs with some fixed number of nodes.  This space has one axis for every parameter that can be altered to give a different network: every parameter that controls each node's internal behavior, along with the connection strengths between different nodes.  Every point in this space specifies one CTRNN; the evolutionary algorithm's job, then, is to find a point in this space that represents a good solution to a given task.  A simple algorithm might, for example, start at one point in parameter space, sample nearby points to see how well they perform at the task, and move toward the best-performing networks.&lt;/p&gt;
&lt;p&gt;Examining the mathematics behind CTRNNs, it turns out that portions of this parameter space represent networks that inherently produce less interesting behavior.  In some areas, the state of every node will always saturate to a stable and unchanging value--not so good for making bugs walk.  In other areas, some or none of the nodes are intrinsically saturated.  My goal was to map out all of these areas, and eventually to use this information to guide evolutionary algorithms toward the most interesting regions of parameter space.&lt;/p&gt;
&lt;p&gt;The parameter space problem turned out to be a complicated mathematical problem that was interesting in its own right, involving combinatorics, probability distributions, and high-dimensional spaces.  In the end, I was able to figure out some useful approximations that make it easier to estimate the percentage of a given parameter space that contains "interesting" CTRNNs.  (The start of a more technical account can be found in this &lt;a href="https://www.public.asu.edu/~bdaniel6/pdfs/CTRNN_draft.pdf"&gt;rough draft&lt;/a&gt;.)  At the tail end of the summer, we discussed possible ways this information could be used to construct smarter evolutionary algorithms, but with busy schedules looming, we didn't get much further.  It remains to be seen whether these seeds will bear any fruit.&lt;/p&gt;</content><category term="What I've Been Thinking About"></category></entry><entry><title>Self-Reliance</title><link href="https://www.public.asu.edu/~bdaniel6/self-reliance.html" rel="alternate"></link><published>2006-08-21T00:00:00-07:00</published><updated>2006-08-21T00:00:00-07:00</updated><author><name>Bryan Daniels</name></author><id>tag:www.public.asu.edu,2006-08-21:/~bdaniel6/self-reliance.html</id><content type="html">&lt;p&gt;&lt;a href="https://www.public.asu.edu/~bdaniel6/pdfs/Self-Reliance.pdf"&gt;Self-Reliance&lt;/a&gt; [pdf]&lt;/p&gt;</content><category term="Other"></category></entry></feed>